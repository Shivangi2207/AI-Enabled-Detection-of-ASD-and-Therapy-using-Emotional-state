{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.0.222-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.2 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (1.26.2)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (2.1.1+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (0.16.1+cu118)\n",
      "Collecting tqdm>=4.64.0 (from ultralytics)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: psutil in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n",
      "Requirement already satisfied: filelock in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.0)\n",
      "Requirement already satisfied: jinja2 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.0.222-py3-none-any.whl (653 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m654.0/654.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, opencv-python, thop, ultralytics\n",
      "Successfully installed opencv-python-4.8.1.78 thop-0.1.1.post2209072238 tqdm-4.66.1 ultralytics-8.0.222\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from torchvision.io import read_video\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression\n",
    "\n",
    "tmpdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 23:51:59.597084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 23:51:59.661889: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 23:51:59.662033: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(device, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideo\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fvcore (from pytorchvideo)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting av (from pytorchvideo)\n",
      "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting parameterized (from pytorchvideo)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting iopath (from pytorchvideo)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: networkx in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from pytorchvideo) (3.0)\n",
      "Requirement already satisfied: numpy in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from fvcore->pytorchvideo) (1.26.2)\n",
      "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from fvcore->pytorchvideo) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from fvcore->pytorchvideo) (4.66.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from fvcore->pytorchvideo) (2.4.0)\n",
      "Requirement already satisfied: Pillow in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from fvcore->pytorchvideo) (10.1.0)\n",
      "Collecting tabulate (from fvcore->pytorchvideo)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/iiitb/lib/miniforge3/envs/py310-tf/lib/python3.10/site-packages (from iopath->pytorchvideo) (4.8.0)\n",
      "Collecting portalocker (from iopath->pytorchvideo)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=f16898be5440dfa0ef162275f3bcf666936eebc034265f2605a6bc0be45d4cb3\n",
      "  Stored in directory: /home/iiitb/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=c852c639c69591f9ce00be850c08d622447156ecb5d6607d71a46e663fb19f5a\n",
      "  Stored in directory: /home/iiitb/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=328e76de98623bc6007dd7080a85733a891607c32a1c7034084f4325fcbc0524\n",
      "  Stored in directory: /home/iiitb/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built pytorchvideo fvcore iopath\n",
      "Installing collected packages: yacs, tabulate, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\n",
      "Successfully installed av-11.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.8.2 pytorchvideo-0.1.5 tabulate-0.9.0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the YOLOv8 model and load weights\n",
    "model =  YOLO('yolov8n.pt')\n",
    "stride = int(model.stride.max())  # model stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold for confidence score and non-maximum suppression\n",
    "conf_threshold = 0.5\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Initialize the video capture\n",
    "video_path = 'path/to/your/video/file.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video details\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize output video writer\n",
    "output_path = 'output.avi'\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MJPG'), fps, (frame_width, frame_height))\n",
    "\n",
    "# Initialize an empty list to store detected objects' coordinates\n",
    "detected_objects = []\n",
    "\n",
    "# Process each frame in the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform YOLOv8 inference\n",
    "    result = model(rgb_frame, size=stride)\n",
    "    pred = non_max_suppression(result, conf_threshold, iou_threshold)[0]\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    if pred is not None:\n",
    "        for det in pred:\n",
    "            x1, y1, x2, y2, conf, cls = det\n",
    "            if int(cls) == 0:  # 'person' class index\n",
    "                detected_objects.append([x1, y1, x2, y2])\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release video capture and writer\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the detected_objects list to a NumPy array\n",
    "detected_objects_array = np.array(detected_objects)\n",
    "\n",
    "# Display or save the NumPy array as needed\n",
    "print(detected_objects_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame selector algorithm and joint location and action localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "\n",
    "MODEL_PATH = 'model/path/here'\n",
    "movenet_model = tf.lite.Interpreter(model_path=MODEL_PATH)\n",
    "movenet_model.allocate_tensors()\n",
    "\n",
    "\n",
    "def get_movenet_data(path):\n",
    "    estimator = tf.lite.Interpreter(model_path='/content/drive/MyDrive/lite-model_movenet_singlepose_thunder_3.tflite')\n",
    "    #estimator = tf.lite.Interpreter(model_path='/content/drive/MyDrive/lite-model_mediapipe_3.tflite')\n",
    "    estimator.allocate_tensors()\n",
    "    \n",
    "    cap = cv.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open video.\")\n",
    "        return [], []\n",
    "\n",
    "    frames = []\n",
    "    vid_keypts = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Preprocess frame for MoveNet model\n",
    "            img1 = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 256, 341)\n",
    "            frames.append(np.squeeze(img1))\n",
    "\n",
    "            # Preprocess frame for keypoint estimation\n",
    "            img = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 256, 256)\n",
    "            input_image = tf.cast(img, dtype=tf.float32)\n",
    "\n",
    "            # Model inference\n",
    "            input_details = estimator.get_input_details()\n",
    "            output_details = estimator.get_output_details()\n",
    "            estimator.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "            estimator.invoke()\n",
    "            keypts = estimator.get_tensor(output_details[0]['index'])\n",
    "            vid_keypts.append(keypts)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return vid_keypts, frames\n",
    "\n",
    "\n",
    "def frame_with_max_change(keypts):\n",
    "    max_diff = 0\n",
    "    max_loc = 0\n",
    "\n",
    "    for frame in range(len(keypts) - 1):\n",
    "        shaped1 = np.squeeze(keypts[frame])\n",
    "        shaped2 = np.squeeze(keypts[frame + 1])\n",
    "\n",
    "        # Calculate the Euclidean distance between consecutive frames\n",
    "        diff_mag = np.linalg.norm(shaped2 - shaped1)\n",
    "\n",
    "        if max_diff < diff_mag:\n",
    "            max_diff = diff_mag\n",
    "            max_loc = frame\n",
    "\n",
    "    return max_diff, max_loc\n",
    "\n",
    "\n",
    "def processed(keypts):\n",
    "    # Extract x and y coordinates and flatten for each frame\n",
    "    return [np.squeeze(k)[:, :2].flatten() for k in keypts]\n",
    "\n",
    "\n",
    "def get_best_frames(video_dir, test=False):\n",
    "    best_frames = []\n",
    "    all_keypts = []\n",
    "    y = []\n",
    "\n",
    "    # Set Label\n",
    "    set_label = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
    "    if 'headbanging' in video_dir and not test:\n",
    "        set_label = np.array([0.0, 1.0, 0.0], dtype=np.float32)\n",
    "    elif 'spinning' in video_dir and not test:\n",
    "        print(\"SPINNING -- TRAIN\")\n",
    "        set_label = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n",
    "\n",
    "    for video in os.listdir(video_dir):\n",
    "        if 'noclass' in video:\n",
    "            continue\n",
    "\n",
    "        keypts, frames = get_movenet_data(os.path.join(video_dir, video))\n",
    "        all_keypts.append(processed(keypts))\n",
    "\n",
    "        _, max_loc = frame_with_max_change(keypts)\n",
    "        best_frames.append(frames[max_loc + 1])\n",
    "\n",
    "        if not test:\n",
    "            y.append(set_label)\n",
    "        else:\n",
    "            # Extract labels for test set\n",
    "            if 'armflapping' in video:\n",
    "                y.append(np.array([1.0, 0.0, 0.0], dtype=np.float32))\n",
    "            elif 'headbanging' in video:\n",
    "                y.append(np.array([0.0, 1.0, 0.0], dtype=np.float32))\n",
    "            elif 'spinning' in video:\n",
    "                y.append(np.array([0.0, 0.0, 1.0], dtype=np.float32))\n",
    "\n",
    "    if test:\n",
    "        test_keys, test_best_frames, test_labels = [], [], []\n",
    "        for frame, keypt, label in zip(best_frames, all_keypts, y):\n",
    "            if len(keypt) == 40:\n",
    "                test_keys.append(keypt)\n",
    "                test_best_frames.append(frame)\n",
    "                test_labels.append(label)\n",
    "\n",
    "        return (np.array(test_best_frames, dtype=np.float32), np.array(test_keys, dtype=np.float32)), np.array(\n",
    "            test_labels, dtype=np.float32)\n",
    "\n",
    "    return (best_frames, all_keypts), y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main model which does classification of ASD and No ASD based on stimming actions detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalSSBDModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Detects the presence and type of stimming behavior using video and Movenet features.\n",
    "\n",
    "    Components:\n",
    "        - Spatiotemporal convolutional block (extracts features from video)\n",
    "        - Bidirectional LSTM with Multihead Attention (processes temporal dynamics)\n",
    "        - Fully-connected network (classifies stimming type)\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of video input channels\n",
    "        intermediate_channels (int): Intermediate channels in the spatiotemporal block\n",
    "        out_channels (int): Output channels from the spatiotemporal block\n",
    "        kernel_size (tuple): Kernel size for both spatial and temporal dimensions\n",
    "        strides (tuple): Strides for both spatial and temporal dimensions\n",
    "        lstm_dim_embedding (int): Dimension of LSTM hidden state\n",
    "        lstm_dim_hidden (int): Dimension of LSTM hidden layer\n",
    "        num_lstm_layers (int): Number of bidirectional LSTM layers\n",
    "        fc_layer_1_dim (int): Dimension of the first fully-connected layer\n",
    "        fc_layer_2_dim (int): Dimension of the second fully-connected layer\n",
    "        n_classes (int): Number of stimming behavior classes\n",
    "        dropout_rate (float): Dropout rate applied in LSTM and Attention block\n",
    "        use_movenet (bool): Flag to use Movenet features (default True)\n",
    "        n_frames (int): Number of video frames used as input (default 40)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, intermediate_channels, out_channels,\n",
    "                 kernel_size, strides, lstm_dim_embedding, lstm_dim_hidden,\n",
    "                 num_lstm_layers, fc_layer_1_dim, fc_layer_2_dim, n_classes,\n",
    "                 dropout_rate=0.2, use_movenet=True, n_frames=40):\n",
    "        super().__init__()\n",
    "\n",
    "        # Spatiotemporal feature extraction\n",
    "        self.spatiotemporal_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, intermediate_channels,\n",
    "                      kernel_size=(1, *kernel_size), stride=(1, *strides), padding='valid'),\n",
    "            nn.BatchNorm3d(intermediate_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(intermediate_channels, out_channels,\n",
    "                      kernel_size=(*kernel_size, 1), stride=(*strides, 1), padding='valid'),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM with Multihead Attention\n",
    "        self.lstm = nn.LSTM(\n",
    "            lstm_dim_embedding + movenet_dim if use_movenet else lstm_dim_embedding,\n",
    "            lstm_dim_hidden, num_layers=num_lstm_layers, dropout=dropout_rate, bidirectional=True)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=lstm_dim_embedding, num_heads=1, dropout=dropout_rate / 4)\n",
    "\n",
    "        # Fully-connected network for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_dim_hidden * 2, fc_layer_1_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(fc_layer_1_dim),\n",
    "            nn.Linear(fc_layer_1_dim, fc_layer_2_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(fc_layer_2_dim),\n",
    "            nn.Linear(fc_layer_2_dim, n_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.use_movenet = use_movenet\n",
    "        self.n_frames = n_frames\n",
    "\n",
    "def forward(self, video, movenet_features=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        video (torch.Tensor): Video frames (shape: [batch_size, n_frames, channels, height, width])\n",
    "        movenet_features (torch.Tensor): Movenet features (optional, shape: [batch_size, n_frames, movenet_dim])\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output logits for stimming type classification (shape: [batch_size, n_classes])\n",
    "    \"\"\"\n",
    "\n",
    "    hidden = None\n",
    "    features_list = []\n",
    "\n",
    "    for frame_idx in range(self.n_frames):\n",
    "        # Extract features from video frame\n",
    "        frame_features = self.spatiotemporal_conv(video[:, frame_idx, :, :, :])\n",
    "\n",
    "        # Combine features with joint location from mediapipe \n",
    "        if self.use_movenet and movenet_features is not None:\n",
    "            combined_features = torch.cat([frame_features, movenet_features[:, frame_idx, :]], dim=1)\n",
    "        else:\n",
    "            combined_features = frame_features\n",
    "\n",
    "        features_list.append(combined_features.unsqueeze(0))  # Add dimension for LSTM\n",
    "\n",
    "    # Pass features through LSTM and Attention\n",
    "    lstm_out, hidden = self.lstm(torch.cat(features_list), hidden)\n",
    "    attn_output, attn_weights = self.attn(lstm_out, lstm_out, lstm_out)\n",
    "\n",
    "    # Use last hidden state for classification\n",
    "    logits = self.fc(attn_output[:, -1, :])\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(video_frames, movenet_features, ssbd_model):\n",
    "    \"\"\"\n",
    "    Classify ASD and No_ASD based on stimming action.\n",
    "\n",
    "    Args:\n",
    "        video_frames (torch.Tensor): Video frames (shape: [batch_size, n_frames, channels, height, width])\n",
    "        movenet_features (torch.Tensor): Movenet features (shape: [batch_size, n_frames, movenet_dim])\n",
    "        ssbd_model (MultimodalSSBDModel): An instance of the MultimodalSSBDModel\n",
    "\n",
    "    Returns:\n",
    "        str: \"ASD\" or \"No_ASD\" based on the classification result\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    ssbd_model.eval()\n",
    "\n",
    "    # Convert video frames and Movenet features to the appropriate device\n",
    "    video_frames = video_frames.to(device=ssbd_model.device)  # TODO\n",
    "    movenet_features = movenet_features.to(device=ssbd_model.device) # TODO\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Call the forward method of MultimodalSSBDModel\n",
    "        logits = ssbd_model(video_frames, movenet_features)\n",
    "\n",
    "        # Perform argmax to get the predicted class\n",
    "        _, predicted_class = torch.max(logits, dim=1)\n",
    "\n",
    "        # Map the predicted class to the corresponding label\n",
    "        class_mapping = {0: \"No_ASD\", 1: \"ASD\"}\n",
    "        predicted_label = class_mapping[predicted_class.item()]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
